{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, PretrainedConfig, PreTrainedModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig, PreTrainedModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import deepchopper\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"{platform.system()=}\")\n",
    "if platform.system() == \"Linux\":\n",
    "    root_dir = Path(\"/projects/b1171/ylk4626/project/DeepChopper\")\n",
    "else:\n",
    "    root_dir = Path(\"/Users/ylk4626/ClionProjects/DeepChopper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = root_dir / \"tests/data/test_input.parquet\"\n",
    "data_files = {\"train\": train_file.as_posix()}\n",
    "\n",
    "num_proc = 8\n",
    "train_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=data_files,\n",
    "    num_proc=num_proc,\n",
    "    split=\"train[:80%]\",\n",
    ").with_format(\"torch\")\n",
    "val_dataset = load_dataset(\n",
    "    \"parquet\", data_files=data_files, num_proc=num_proc, split=\"train[80%:90%]\"\n",
    ").with_format(\"torch\")\n",
    "test_dataset = load_dataset(\n",
    "    \"parquet\", data_files=data_files, num_proc=num_proc, split=\"train[90%:]\"\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"train_dataset: {train_dataset}\")\n",
    "print(f\"val_dataset: {val_dataset}\")\n",
    "print(f\"test_dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_from_hyena_model(model_name):\n",
    "    max_lengths = {\n",
    "        \"hyenadna-tiny-1k-seqlen\": 1024,\n",
    "        \"hyenadna-small-32k-seqlen\": 32768,\n",
    "        \"hyenadna-medium-160k-seqlen\": 160000,\n",
    "        \"hyenadna-medium-450k-seqlen\": 450000,  # T4 up to here\n",
    "        \"hyenadna-large-1m-seqlen\": 1_000_000,  # only A100 (paid tier)\n",
    "    }\n",
    "\n",
    "    if model_name not in max_lengths:\n",
    "        msg = f\"Model name {model_name} not found in available models.\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    max_length = max_lengths[model_name]\n",
    "    # bfloat16 for better speed and reduced memory usage\n",
    "    model_name = f\"LongSafari/{model_name}-hf\"\n",
    "    return AutoTokenizer.from_pretrained(\n",
    "        model_name, max_length=max_length, truncation=True, padding=True, trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyenadna_name = \"hyenadna-small-32k-seqlen\"\n",
    "tokenizer = load_tokenizer_from_hyena_model(hyenadna_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchopper.models.hyena import (  # DataCollatorForTokenClassificationWithQual,; tokenize_and_align_labels_and_quals,\n",
    "    IGNORE_INDEX,\n",
    "    compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "import deepchopper\n",
    "\n",
    "\n",
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\"\"\"\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "class DataCollatorForTokenClassificationWithQual(DataCollatorForTokenClassification):\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n",
    "        labels = (\n",
    "            [feature[label_name] for feature in features] if label_name in features[0] else None\n",
    "        )\n",
    "\n",
    "        qual_name = \"input_quals\"\n",
    "        qual_pad_token_id = 0\n",
    "        input_quals = [feature[qual_name] for feature in features]\n",
    "\n",
    "        no_labels_features = [\n",
    "            {k: v for k, v in feature.items() if k not in [qual_name, label_name]}\n",
    "            for feature in features\n",
    "        ]\n",
    "\n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer,\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "\n",
    "        if padding_side == \"right\":\n",
    "            batch[label_name] = [\n",
    "                to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label))\n",
    "                for label in labels\n",
    "            ]\n",
    "            batch[qual_name] = [\n",
    "                to_list(qual) + [qual_pad_token_id] * (sequence_length - len(qual))\n",
    "                for qual in input_quals\n",
    "            ]\n",
    "        else:\n",
    "            batch[label_name] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label)\n",
    "                for label in labels\n",
    "            ]\n",
    "            batch[qual_name] = [\n",
    "                [qual_pad_token_id] * (sequence_length - len(qual)) + to_list(qual)\n",
    "                for qual in input_quals\n",
    "            ]\n",
    "\n",
    "        batch[label_name] = torch.tensor(batch[label_name], dtype=torch.int64)\n",
    "        batch[qual_name] = torch.tensor(batch[qual_name], dtype=torch.float32)\n",
    "        print(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = DataCollatorForTokenClassificationWithQual(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels_and_quals(\n",
    "    data, tokenizer, max_length, pad_qual=0, pad_label=IGNORE_INDEX\n",
    "):\n",
    "    tokenized_inputs = tokenizer(data[\"seq\"], max_length=max_length, truncation=True, padding=True)\n",
    "    labels = torch.tensor(\n",
    "        [*deepchopper.vertorize_target(*data[\"target\"], len(data[\"seq\"])), pad_label]\n",
    "    )\n",
    "    quals = torch.cat((data[\"qual\"], torch.tensor([pad_qual]))).float()\n",
    "    normalized_quals = torch.nn.functional.normalize(quals, dim=0)\n",
    "    tokenized_inputs.update({\"labels\": labels, \"input_quals\": normalized_quals})\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    partial(\n",
    "        tokenize_and_align_labels_and_quals,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=tokenizer.max_len_single_sentence,\n",
    "    ),\n",
    "    batched=False,\n",
    "    num_proc=12,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ").remove_columns([\"id\", \"seq\", \"qual\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "dataloader = DataLoader(tokenized_train_dataset, batch_size=4, collate_fn=tt.torch_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_quals\"][2][-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_quals\"][2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicsBenchmarkCNN(nn.Module):\n",
    "    def __init__(self, number_of_classes, vocab_size, input_len, embedding_dim=100):\n",
    "        \"\"\"Genomics Benchmark CNN model.\n",
    "\n",
    "        `embedding_dim` = 100 comes from:\n",
    "        https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/tree/main/experiments/torch_cnn_experiments\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=16, kernel_size=8, bias=True),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(in_channels=16, out_channels=8, kernel_size=8, bias=True),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(in_channels=8, out_channels=4, kernel_size=8, bias=True),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.dense_model = nn.Sequential(\n",
    "            nn.Linear(self.count_flatten_size(input_len), 512),\n",
    "            # To be consistent with SSM classifier decoders, we use num_classes (even when it's binary)\n",
    "            nn.Linear(512, number_of_classes),\n",
    "        )\n",
    "\n",
    "    def count_flatten_size(self, input_len):\n",
    "        zeros = torch.zeros([1, input_len], dtype=torch.long)\n",
    "        x = self.embeddings(zeros)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.cnn_model(x)\n",
    "        return x.size()[1]\n",
    "\n",
    "    def forward(self, x, state=None):  # Adding `state` to be consistent with other models\n",
    "        x = self.embeddings(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.cnn_model(x)\n",
    "        x = self.dense_model(x)\n",
    "        return x, state  # Returning tuple to be consistent with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc",
   "language": "python",
   "name": "dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
