{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ce01b-01e4-4ee2-bcc2-91bdf3a0dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, PretrainedConfig, PreTrainedModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eee996-6e01-476a-a91d-cb2f76bc3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, PretrainedConfig, PreTrainedModel,AutoTokenizer\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.utils import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1cd2f9-2c6c-4a82-841f-dd2ad8e5cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import deepchopper\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42123f-70c5-4fba-993f-3b3dc59e4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform \n",
    "from pathlib import Path \n",
    "\n",
    "print(f\"{platform.system()=}\")\n",
    "if platform.system() == \"Linux\":\n",
    "    root_dir = Path(\"/projects/b1171/ylk4626/project/DeepChopper\")\n",
    "else:\n",
    "    root_dir = Path(\"/Users/ylk4626/ClionProjects/DeepChopper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e83626-73bf-4870-b0ab-e311324e3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = root_dir / \"tests/data/test_input.parquet\"\n",
    "data_files = {\"train\": train_file.as_posix()}\n",
    "\n",
    "num_proc = 8\n",
    "train_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=data_files,\n",
    "    num_proc=num_proc,\n",
    "    split=\"train[:80%]\",\n",
    ").with_format(\"torch\")\n",
    "val_dataset = load_dataset(\n",
    "    \"parquet\", data_files=data_files, num_proc=num_proc, split=\"train[80%:90%]\"\n",
    ").with_format(\"torch\")\n",
    "test_dataset = load_dataset(\n",
    "    \"parquet\", data_files=data_files, num_proc=num_proc, split=\"train[90%:]\"\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"train_dataset: {train_dataset}\")\n",
    "print(f\"val_dataset: {val_dataset}\")\n",
    "print(f\"test_dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c1d16-6380-4031-a0ab-81f727898b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d030c-2407-46ce-bff4-89b13b03bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ed361-43e3-42e2-b483-3d7509338856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716dc500-d222-4461-ac18-4415aad59cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_from_hyena_model(model_name):\n",
    "    max_lengths = {\n",
    "        \"hyenadna-tiny-1k-seqlen\": 1024,\n",
    "        \"hyenadna-small-32k-seqlen\": 32768,\n",
    "        \"hyenadna-medium-160k-seqlen\": 160000,\n",
    "        \"hyenadna-medium-450k-seqlen\": 450000,  # T4 up to here\n",
    "        \"hyenadna-large-1m-seqlen\": 1_000_000,  # only A100 (paid tier)\n",
    "    }\n",
    "\n",
    "    if model_name not in max_lengths:\n",
    "        msg = f\"Model name {model_name} not found in available models.\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    max_length = max_lengths[model_name]\n",
    "    # bfloat16 for better speed and reduced memory usage\n",
    "    model_name = f\"LongSafari/{model_name}-hf\"\n",
    "    return AutoTokenizer.from_pretrained(\n",
    "        model_name, max_length=max_length, truncation=True, padding=True, trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f367f9-0451-4edc-97e5-11b0ee794b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyenadna_name = \"hyenadna-small-32k-seqlen\"\n",
    "tokenizer = load_tokenizer_from_hyena_model(hyenadna_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ade6f-0061-4b23-91e6-2b303ad8d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchopper.models.hyena import (\n",
    "    IGNORE_INDEX,\n",
    "    # DataCollatorForTokenClassificationWithQual,\n",
    "    compute_metrics,\n",
    "    # tokenize_and_align_labels_and_quals,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4310f5ca-a644-4e7b-96e7-21f4a6e0f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "import deepchopper\n",
    "\n",
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\"\"\"\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "class DataCollatorForTokenClassificationWithQual(DataCollatorForTokenClassification):\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n",
    "        labels = (\n",
    "            [feature[label_name] for feature in features] if label_name in features[0] else None\n",
    "        )\n",
    "\n",
    "        qual_name = \"input_quals\"\n",
    "        qual_pad_token_id = 0\n",
    "        input_quals = [feature[qual_name] for feature in features]\n",
    "\n",
    "        no_labels_features = [\n",
    "            {k: v for k, v in feature.items() if k not in [qual_name, label_name]}\n",
    "            for feature in features\n",
    "        ]\n",
    "\n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer,\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "        \n",
    "        if padding_side == \"right\":\n",
    "            batch[label_name] = [\n",
    "                to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label))\n",
    "                for label in labels\n",
    "            ]\n",
    "            batch[qual_name] = [\n",
    "                to_list(qual) + [qual_pad_token_id] * (sequence_length - len(qual))\n",
    "                for qual in input_quals\n",
    "            ]\n",
    "        else:\n",
    "            batch[label_name] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label)\n",
    "                for label in labels\n",
    "            ]\n",
    "            batch[qual_name] = [\n",
    "                [qual_pad_token_id] * (sequence_length - len(qual)) + to_list(qual)\n",
    "                for qual in input_quals\n",
    "            ]\n",
    "\n",
    "        batch[label_name] = torch.tensor(batch[label_name], dtype=torch.int64)\n",
    "        batch[qual_name] = torch.tensor(batch[qual_name], dtype=torch.float32)\n",
    "        print(batch)        \n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ecc52f65-8aa5-4218-bcdf-03b2698ac0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tt = DataCollatorForTokenClassificationWithQual(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d7cd0fc6-eb29-43d7-9736-c4fcca98f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels_and_quals(\n",
    "    data, tokenizer, max_length, pad_qual=0, pad_label=IGNORE_INDEX\n",
    "):\n",
    "    tokenized_inputs = tokenizer(data[\"seq\"], max_length=max_length, truncation=True, padding=True)\n",
    "    labels = torch.tensor(\n",
    "        [*deepchopper.vertorize_target(*data[\"target\"], len(data[\"seq\"])), pad_label]\n",
    "    )\n",
    "    quals = torch.cat((data[\"qual\"], torch.tensor([pad_qual]))).float()\n",
    "    normalized_quals = torch.nn.functional.normalize(quals, dim=0)\n",
    "    tokenized_inputs.update({\"labels\": labels, \"input_quals\": normalized_quals})\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b64394c-ee6d-46e4-9c03-d63741e2d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/b1171/ylk4626/mambaforge/envs/deepchopper/lib/python3.10/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class 'builtins.FqEncoderOption'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/projects/b1171/ylk4626/mambaforge/envs/deepchopper/lib/python3.10/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class 'builtins.FqEncoderOption'>: builtins.FqEncoderOption has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9bd09485b043cb95a52a4f02ddb017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset (num_proc=12):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "                    partial(\n",
    "                        tokenize_and_align_labels_and_quals,\n",
    "                        tokenizer=tokenizer,\n",
    "                        max_length=tokenizer.max_len_single_sentence,\n",
    "                    ),\n",
    "                    batched=False,\n",
    "                    num_proc=12,\n",
    "                    desc=\"Running tokenizer on train dataset\",\n",
    "                ).remove_columns([\"id\", \"seq\", \"qual\", \"target\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7a936252-1584-4ab8-b1e5-06e751b81556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([9, 8, 7,  ..., 8, 7, 1]),\n",
       " 'labels': tensor([   0,    0,    0,  ...,    0,    0, -100]),\n",
       " 'input_quals': tensor([0.0098, 0.0113, 0.0210,  ..., 0.0188, 0.0218, 0.0000])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a8be2053-f21b-48b8-b412-4bbb38a010b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "dataloader = DataLoader(tokenized_train_dataset, batch_size=4, collate_fn=tt.torch_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a5b09b4b-e067-4611-90c8-b565d07af655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 4,  4,  4,  ...,  8,  7,  1],\n",
      "        [ 4,  4,  4,  ...,  9,  7,  1],\n",
      "        [ 9, 10,  7,  ..., 10,  8,  1],\n",
      "        [ 4,  4,  4,  ...,  7,  7,  1]]), 'labels': tensor([[-100, -100, -100,  ...,    0,    0, -100],\n",
      "        [-100, -100, -100,  ...,    0,    0, -100],\n",
      "        [   0,    0,    0,  ...,    0,    0, -100],\n",
      "        [-100, -100, -100,  ...,    0,    0, -100]]), 'input_quals': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0218, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0228, 0.0000],\n",
      "        [0.0141, 0.0196, 0.0201,  ..., 0.0087, 0.0082, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0299, 0.0276, 0.0000]])}\n",
      "{'input_ids': tensor([[ 4,  4,  4,  ...,  8,  7,  1],\n",
      "        [ 4,  4,  4,  ...,  9,  7,  1],\n",
      "        [ 9, 10,  7,  ..., 10,  8,  1],\n",
      "        [ 4,  4,  4,  ...,  7,  7,  1]]), 'labels': tensor([[-100, -100, -100,  ...,    0,    0, -100],\n",
      "        [-100, -100, -100,  ...,    0,    0, -100],\n",
      "        [   0,    0,    0,  ...,    0,    0, -100],\n",
      "        [-100, -100, -100,  ...,    0,    0, -100]]), 'input_quals': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0218, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0228, 0.0000],\n",
      "        [0.0141, 0.0196, 0.0201,  ..., 0.0087, 0.0082, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0299, 0.0276, 0.0000]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac99468-5c1a-4f6a-ae55-06160f29df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e164f-2269-44d6-af27-ff63cb9c85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea5c45-6104-4dce-92bb-58c53eb9d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_quals'][2][-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99bc78-b84b-482b-b150-56a936afab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_quals'][2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628ce29-89d2-4596-aed9-8a70415e3f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc",
   "language": "python",
   "name": "dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
