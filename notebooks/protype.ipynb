{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "import deepchopper\n",
    "from deepchopper.models import KmerPreTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import NormalizedString, PreTokenizedString, Regex, Tokenizer\n",
    "from tokenizers.decoders import Decoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Normalizer\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "\n",
    "import deepchopper\n",
    "\n",
    "\n",
    "def compute_tokens_to_ids(kmer_size: int) -> tuple[dict[str, int], list[str]]:\n",
    "    kmers_tokens = deepchopper.generate_kmers(deepchopper.default.BASES, kmer_size)\n",
    "    standard_tokens = kmers_tokens\n",
    "\n",
    "    unknown_token = \"<UNK>\"\n",
    "    padding_token = \"<PAD>\"\n",
    "    eos_token = \"<EOS>\"\n",
    "    bos_token = \"<BOS>\"\n",
    "    sep_token = \"<SEP>\"\n",
    "\n",
    "    specical_tokens = [padding_token, unknown_token, eos_token, bos_token, sep_token]\n",
    "    all_tokens = standard_tokens + specical_tokens\n",
    "    tokens_to_ids = {tok: i for i, tok in enumerate(all_tokens)}\n",
    "    return tokens_to_ids, all_tokens\n",
    "\n",
    "\n",
    "class KmerPreTokenizer:\n",
    "    def __init__(self, kmer_size: int, *, overlap: bool):\n",
    "        self.kmer_size = kmer_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def kmer_split(self, i: int, normalized_string: NormalizedString) -> list[NormalizedString]:\n",
    "        return [\n",
    "            normalized_string[start:end]\n",
    "            for (_token, (start, end)) in deepchopper.seq_to_kmers_and_offset(\n",
    "                sequence, self.kmer_size, self.overlap\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        # Let's call split on the PreTokenizedString to split using `self.jieba_split`\n",
    "        pretok.split(self.kmer_split)\n",
    "\n",
    "\n",
    "class KmerDecoder:\n",
    "    def decode(self, tokens: list[str]) -> str:\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.text import Text\n",
    "\n",
    "\n",
    "def hight_text(text: str, start: int, end: int):\n",
    "    text = Text(text)\n",
    "    console = Console()\n",
    "    text.stylize(\"bold magenta\", start, end)\n",
    "    console.print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pre_tokenize_str_no_overlap():\n",
    "    tokenizer = KmerPreTokenizer(3, overlap=False)\n",
    "    sequence = \"ATCGGCC\"\n",
    "    expected_output = [(\"ATC\", (0, 3)), (\"GGC\", (3, 6))]\n",
    "    res = tokenizer.pre_tokenize_str(sequence)\n",
    "    assert res == expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"../tests/data/test_input.parquet\"}\n",
    "num_proc = 8\n",
    "train_dataset = load_dataset(\n",
    "    \"parquet\", data_files=data_files, num_proc=num_proc, split=\"train[:70%]\"\n",
    ")\n",
    "val_dataset = load_dataset(\n",
    "    \"parquet\", data_files=data_files, num_proc=num_proc, split=\"train[70%:90%]\"\n",
    ")\n",
    "test_dataset = load_dataset(\n",
    "    \"parquet\", data_files=data_files, num_proc=num_proc, split=\"train[90%:]\"\n",
    ")\n",
    "\n",
    "print(f\"train_dataset: {train_dataset}\")\n",
    "print(f\"val_dataset: {val_dataset}\")\n",
    "print(f\"test_dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[\"seq\"][0]\n",
    "train_dataset[\"id\"][0]\n",
    "# train_dataset['qual'][0]\n",
    "train_dataset[\"target\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hight_text(train_dataset[\"seq\"][0], *train_dataset[\"target\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepchopper.seq_to_kmers(train_dataset['seq'][0], 5, overlap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.map(lambda x : partial(deepchopper.seq_to_kmers, overlap=False, k=5)(x['seq']))\n",
    "# test_dataset.map(lambda x : print(x['seq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel())\n",
    "tokenizer.pre_tokenizer = PreTokenizer.custom(KmerPreTokenizer(3, overlap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = train_dataset[\"seq\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d9776505-2188-4595-962e-1a7e49c1d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_train_dataset = train_dataset.map(lambda x : tokenizer(x['seq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cdf61e14-8406-437a-acbf-37654e2150fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_len  = [ len(i) for i in encode_train_dataset['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c84c5777-586e-4884-8f3d-bba35cf861fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2247"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(input_ids_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5db93d-ef70-4986-8e85-bfef9403988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4533db0d-d5f5-4917-9f7a-5ee35f31a6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'GCAGCTA',\n",
       " 'TGAATG',\n",
       " 'CAA',\n",
       " 'GGCCA',\n",
       " 'CAAGGTG',\n",
       " 'GATGGAA',\n",
       " 'GAGTT',\n",
       " 'GTGGAA',\n",
       " 'CCAAA',\n",
       " 'GAGCTG',\n",
       " 'TCTTCCA',\n",
       " 'GAGAA',\n",
       " 'GATT',\n",
       " 'TCGAGA',\n",
       " 'TAAGTC',\n",
       " 'GCC',\n",
       " 'CATCA',\n",
       " 'GTGAA',\n",
       " 'CAAGA',\n",
       " 'TATTGTT',\n",
       " 'GGTG',\n",
       " 'GCATT',\n",
       " 'TGA',\n",
       " 'TGAGAA',\n",
       " 'CGTT',\n",
       " 'CCAA',\n",
       " 'GATTATT',\n",
       " 'GACAGA',\n",
       " 'TTA',\n",
       " 'GTGAAAA',\n",
       " 'GTAA',\n",
       " 'GATT',\n",
       " 'GAAA',\n",
       " 'TCATGA',\n",
       " 'CTGA',\n",
       " 'CCGTAA',\n",
       " 'GTGGCAA',\n",
       " 'GAAAGG',\n",
       " 'GCTTTT',\n",
       " 'GCCTTTG',\n",
       " 'TAACCTT',\n",
       " 'TGACGA',\n",
       " 'CCATGA',\n",
       " 'CTCC',\n",
       " 'GTG',\n",
       " 'GATAA',\n",
       " 'GATT',\n",
       " 'GTCA',\n",
       " 'TTCA',\n",
       " 'GAA',\n",
       " 'TACCA',\n",
       " 'TACTG',\n",
       " 'TGAATG',\n",
       " 'GCCACA',\n",
       " 'TCTTTATT',\n",
       " 'GTGAA',\n",
       " 'GTTA',\n",
       " 'GAAAA',\n",
       " 'GCCCTG',\n",
       " 'TCAAA',\n",
       " 'GCAA',\n",
       " 'GAGA',\n",
       " 'TGAA',\n",
       " 'TCAGTG',\n",
       " 'CTT',\n",
       " 'CTCCAGCC',\n",
       " 'AAA',\n",
       " 'GAGG',\n",
       " 'TCGAA',\n",
       " 'GTG',\n",
       " 'GTTCTG',\n",
       " 'GAAA',\n",
       " 'CTTTG',\n",
       " 'GTGGTG',\n",
       " 'GTCGTG',\n",
       " 'GAGGTG',\n",
       " 'GTT',\n",
       " 'TCGGTG',\n",
       " 'GGAA',\n",
       " 'TGACAA',\n",
       " 'CTCGG',\n",
       " 'TCGTG',\n",
       " 'GAGGAAA',\n",
       " 'CTT',\n",
       " 'CAGTG',\n",
       " 'GTC',\n",
       " 'GTGGTG',\n",
       " 'GCTTTG',\n",
       " 'GTGGCA',\n",
       " 'GCC',\n",
       " 'GTGGTG',\n",
       " 'GTGGTG',\n",
       " 'GATATG',\n",
       " 'GTGGCA',\n",
       " 'GTGGG',\n",
       " 'GATG',\n",
       " 'GCTA',\n",
       " 'TAATG',\n",
       " 'GATTTG',\n",
       " 'GTAA',\n",
       " 'TGATG',\n",
       " 'GAAGCAA',\n",
       " 'TTTG',\n",
       " 'GAGGTG',\n",
       " 'GTGGAA',\n",
       " 'GCTA',\n",
       " 'CAA',\n",
       " 'TGATTTT',\n",
       " 'GGGAA',\n",
       " 'TTA',\n",
       " 'CAACAA',\n",
       " 'TCA',\n",
       " 'GTCTT',\n",
       " 'CAAATTTT',\n",
       " 'GGA',\n",
       " 'CCCC',\n",
       " 'TAGGA',\n",
       " 'GGAAA',\n",
       " 'TTTT',\n",
       " 'GGTA',\n",
       " 'GAA',\n",
       " 'GCTCTG',\n",
       " 'GCC',\n",
       " 'CCATG',\n",
       " 'GCGGTG',\n",
       " 'GAGG',\n",
       " 'CCAAA',\n",
       " 'TACTTTT',\n",
       " 'GCAAA',\n",
       " 'CCA',\n",
       " 'CGAAA',\n",
       " 'CCAA',\n",
       " 'GGTG',\n",
       " 'GCTATG',\n",
       " 'GCGG',\n",
       " 'TCCA',\n",
       " 'GCAGCA',\n",
       " 'GCAGTA',\n",
       " 'GCTATG',\n",
       " 'GCAGTG',\n",
       " 'GCA',\n",
       " 'GAAGA',\n",
       " 'TTTTAA',\n",
       " 'TTA',\n",
       " 'GGAAA',\n",
       " 'CAAA',\n",
       " 'GCTTA',\n",
       " 'GCAGGA',\n",
       " 'GAGGA',\n",
       " 'GAGC',\n",
       " 'CAGAGAA',\n",
       " 'GTGA',\n",
       " 'CAGG',\n",
       " 'GAAGTA',\n",
       " 'CAGG',\n",
       " 'TTACAA',\n",
       " 'CAGATT',\n",
       " 'TGTGAA',\n",
       " 'CTCAGCC',\n",
       " 'CAAGCA',\n",
       " 'CAGTG',\n",
       " 'GTG',\n",
       " 'GCAGG',\n",
       " 'GCCTA',\n",
       " 'GCTG',\n",
       " 'CTACAAA',\n",
       " 'GAAGA',\n",
       " 'CATG',\n",
       " 'TTTTA',\n",
       " 'GACAAA',\n",
       " 'TACTCA',\n",
       " 'TGTGTATG',\n",
       " 'GGCAAAA',\n",
       " 'CTT',\n",
       " 'GAGGA',\n",
       " 'CTG',\n",
       " 'TATTTG',\n",
       " 'TGACTAA',\n",
       " 'CTG',\n",
       " 'TATAA',\n",
       " 'CAGG',\n",
       " 'TTATTTTA',\n",
       " 'GTT',\n",
       " 'TCTGTT',\n",
       " 'TGTG',\n",
       " 'GAAA',\n",
       " 'GTGTAAA',\n",
       " 'GCATT',\n",
       " 'CCAA',\n",
       " 'CAAA',\n",
       " 'GGTTTT',\n",
       " 'TAATG',\n",
       " 'TAGA',\n",
       " 'TTTTTTTT',\n",
       " 'TTTG',\n",
       " 'CACC',\n",
       " 'CCATG',\n",
       " 'CTGTT',\n",
       " 'GATTTG',\n",
       " 'CTAAATG',\n",
       " 'TAA',\n",
       " 'CAGTC',\n",
       " 'TGA',\n",
       " 'TCGTGA',\n",
       " 'CGC',\n",
       " 'TGAA',\n",
       " 'TAAATG',\n",
       " 'TCTTTTTT',\n",
       " 'AAAAAAAAAAAAAA',\n",
       " 'GCTCC',\n",
       " 'CTCC',\n",
       " 'CATCC',\n",
       " 'CCTGCTG',\n",
       " 'CTAA',\n",
       " 'CTGA',\n",
       " 'TCC',\n",
       " 'CATTA',\n",
       " 'TATCTAA',\n",
       " 'CCTG',\n",
       " 'CCCC',\n",
       " 'CCCA',\n",
       " 'TATCA',\n",
       " 'CCTG',\n",
       " 'CTCC',\n",
       " 'CGA',\n",
       " 'GCTA',\n",
       " 'CCTAA',\n",
       " 'GAA',\n",
       " 'CAGC',\n",
       " 'TAAAA',\n",
       " 'GAGCA',\n",
       " 'CACC',\n",
       " 'CGCA',\n",
       " 'TGTA',\n",
       " 'GCAAAA',\n",
       " 'TAGTG',\n",
       " 'GGAA',\n",
       " 'GATTA',\n",
       " 'TAGG',\n",
       " 'TAGAGG',\n",
       " 'CGA',\n",
       " 'CAAA',\n",
       " 'CCTA',\n",
       " 'CCGA',\n",
       " 'GCCTG',\n",
       " 'GTGA',\n",
       " 'TAGCTG',\n",
       " 'GTTGTCC',\n",
       " 'TAGA',\n",
       " 'TAGAA',\n",
       " 'TCTTA',\n",
       " 'GTTCAA',\n",
       " 'CTTTAAA',\n",
       " 'TTTG',\n",
       " 'CCCACA',\n",
       " 'GAA',\n",
       " 'CCCTC',\n",
       " 'TAAA',\n",
       " 'TCCCCTT',\n",
       " 'GTAAATT',\n",
       " 'TAACTG',\n",
       " 'TTA',\n",
       " 'GTCCAAA',\n",
       " 'GAGGAA',\n",
       " 'CAGC',\n",
       " 'TCTTTG',\n",
       " 'GACA',\n",
       " 'CTAGG',\n",
       " 'AAAAAA',\n",
       " 'CCTT',\n",
       " 'GTAGAGA',\n",
       " 'GTAAAAAA',\n",
       " 'TCAA',\n",
       " 'CACCCA',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encode_train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebf011e1-0e16-4bbf-9657-000d2e9f219a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dnabert2_117M/tokenizer_config.json',\n",
       " './dnabert2_117M/special_tokens_map.json',\n",
       " './dnabert2_117M/tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./dnabert2_117M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 4085, 513, 27, 229, 2886, 3551, 222, 671, 131, 728, 1403, 145, 73, 1154, 2482, 36, 197, 135, 421, 1310, 103, 183, 23, 430, 134, 76, 1973, 634, 24, 1033, 68, 73, 45, 949, 59, 3153, 2595, 2219, 301, 1697, 2246, 2470, 1008, 78, 30, 250, 73, 67, 115, 25, 268, 236, 513, 460, 2870, 135, 77, 85, 519, 107, 66, 50, 52, 574, 29, 1027, 18, 71, 480, 30, 535, 45, 157, 423, 2855, 719, 31, 1848, 57, 572, 3189, 539, 901, 29, 176, 41, 423, 532, 674, 36, 423, 423, 948, 674, 281, 83, 105, 185, 605, 68, 188, 1689, 86, 719, 671, 105, 27, 703, 200, 24, 361, 22, 161, 2436, 33, 102, 314, 128, 21, 138, 25, 365, 36, 249, 1317, 71, 131, 1543, 124, 34, 409, 76, 103, 772, 247, 82, 349, 918, 772, 515, 32, 330, 182, 24, 128, 49, 263, 595, 191, 212, 1182, 61, 53, 860, 53, 1028, 410, 295, 3818, 801, 176, 30, 253, 628, 62, 1702, 330, 65, 69, 610, 1300, 3883, 1800, 29, 191, 28, 296, 2389, 28, 170, 53, 2356, 31, 319, 42, 45, 2145, 183, 76, 49, 568, 185, 98, 206, 86, 140, 249, 146, 605, 1523, 20, 369, 23, 1518, 118, 52, 318, 1082, 3227, 241, 78, 293, 1567, 79, 59, 26, 213, 3954, 74, 102, 95, 211, 74, 78, 93, 105, 297, 25, 181, 58, 208, 140, 159, 106, 291, 285, 57, 260, 99, 1461, 93, 49, 114, 1396, 192, 61, 1389, 3541, 98, 207, 154, 538, 1177, 86, 698, 25, 372, 38, 1554, 995, 663, 24, 1497, 534, 181, 353, 100, 681, 142, 64, 1437, 1809, 56, 1162, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c88a42c9-29ca-4a90-995c-adf3f9ab53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test_dataset = test_dataset.map(lambda x : tokenizer(x['seq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a4700ab-2d04-4120-82ce-21bca9e3b31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq', 'qual', 'target'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7449f5ec-d0c2-4bee-afae-e65148d91305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq', 'qual', 'target', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aed53fe9-84c0-46e1-9085-d6f6faf2c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1, ts2 = ts[:1024], ts[1024:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "36d197f6-9ac3-43d3-9b96-421fa138ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = file_tokenizer.encode(ts1, ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a0191b38-59b6-4100-8751-d95628647876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mfile_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Encode the given sequence and pair. This method can process raw text sequences\n",
       "as well as already pre-tokenized sequences.\n",
       "\n",
       "Example:\n",
       "    Here are some examples of the inputs that are accepted::\n",
       "\n",
       "        encode(\"A single sequence\")`\n",
       "        encode(\"A sequence\", \"And its pair\")`\n",
       "        encode([ \"A\", \"pre\", \"tokenized\", \"sequence\" ], is_pretokenized=True)`\n",
       "        encode(\n",
       "            [ \"A\", \"pre\", \"tokenized\", \"sequence\" ], [ \"And\", \"its\", \"pair\" ],\n",
       "            is_pretokenized=True\n",
       "        )\n",
       "\n",
       "Args:\n",
       "    sequence (:obj:`~tokenizers.InputSequence`):\n",
       "        The main input sequence we want to encode. This sequence can be either raw\n",
       "        text or pre-tokenized, according to the ``is_pretokenized`` argument:\n",
       "\n",
       "        - If ``is_pretokenized=False``: :class:`~tokenizers.TextInputSequence`\n",
       "        - If ``is_pretokenized=True``: :class:`~tokenizers.PreTokenizedInputSequence`\n",
       "\n",
       "    pair (:obj:`~tokenizers.InputSequence`, `optional`):\n",
       "        An optional input sequence. The expected format is the same that for ``sequence``.\n",
       "\n",
       "    is_pretokenized (:obj:`bool`, defaults to :obj:`False`):\n",
       "        Whether the input is already pre-tokenized\n",
       "\n",
       "    add_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
       "        Whether to add the special tokens\n",
       "\n",
       "Returns:\n",
       "    :class:`~tokenizers.Encoding`: The encoded result\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_tokenizer.encode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ed29e1f9-3d7f-42fc-8a02-736c7c06b2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cb27a-0dfb-4479-864f-9c6a36fc7256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9f1d3f4-8d11-45cf-b309-1b66d6e74a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TCGTGACGCTGAATAAATGTCTTTTTTAAAAAAAAAAAAAAGCTCCCTCCCATCCCCTGCTGCTAACTGATCCCATTATATCTAACCTGCCCCCCCATATCACCTGCTCCCGAGCTACCTAAGAACAGCTAAAAGAGCACACCCGCATGTAGCAAAATAGTGGGAAGATTATAGGTAGAGGCGACAAACCTACCGAGCCTGGTGATAGCTGGTTGTCCTAGATAGAATCTTAGTTCAACTTTAAATTTGCCCACAGAACCCTCTAAATCCCCTTGTAAATTTAACTGTTAGTCCAAAGAGGAACAGCTCTTTGGACACTAGGAAAAAACCTTGTAGAGAGTAAAAAATCAACACCCA'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "25920c62-6205-49e4-91ae-f0a0fc95125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_seq = \"TCGTGACGCTGAATAAATGTCTTTTTTAAAAAAAAAAAAAA\"\n",
    "a1, a2 = \"TCGTGACGCTGAATAAATGTCTT\", \"TTTTAAAAAAAAAAAAAA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b0167f2-623b-48b0-85e5-fd67173f8641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'TCGTGA',\n",
       " 'CGC',\n",
       " 'TGAA',\n",
       " 'TAAATG',\n",
       " 'TCTT',\n",
       " '[SEP]',\n",
       " 'TTTT',\n",
       " 'AAAAAAAAAAAAAA',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tokenizer.encode(a1,a2).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e7d6a69a-5099-4a49-85be-b9d2b60a4baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'TCGTGA', 'CGC', 'TGAA', 'TAAATG', 'TCTT', '[SEP]']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tokenizer.encode(a1).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "53f85404-220e-4889-b220-dc226beb79cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'TTTT', 'AAAAAAAAAAAAAA', '[SEP]']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tokenizer.encode(a2).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b33cb0-4a07-4759-b142-f55ee9c6f0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6342b38-4ed9-4d7b-bc70-da7977fa0266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6a3a1-7b21-48e4-8143-b9db88fd86be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e8519c21-8e71-4ab8-930c-6ad04e60b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tokenizer = tokenizers.Tokenizer.from_file(\"dnabert2_117M/tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6800211-e0b1-4d36-9449-13c1872fdc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mfile_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Add the given special tokens to the Tokenizer.\n",
       "\n",
       "If these tokens are already part of the vocabulary, it just let the Tokenizer know about\n",
       "them. If they don't exist, the Tokenizer creates them, giving them a new id.\n",
       "\n",
       "These special tokens will never be processed by the model (ie won't be split into\n",
       "multiple tokens), and they can be removed from the output when decoding.\n",
       "\n",
       "Args:\n",
       "    tokens (A :obj:`List` of :class:`~tokenizers.AddedToken` or :obj:`str`):\n",
       "        The list of special tokens we want to add to the vocabulary. Each token can either\n",
       "        be a string or an instance of :class:`~tokenizers.AddedToken` for more\n",
       "        customization.\n",
       "\n",
       "Returns:\n",
       "    :obj:`int`: The number of tokens that were created in the vocabulary\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_tokenizer.add_special_tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97ea90a8-0fff-4839-a2dc-3f47bf9ecb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq', 'qual', 'target'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b995627d-6b83-48f3-aeaa-7790ef13cf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=276, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tokenizer.encode(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427240f-4a40-49fd-8493-f1ef5584f84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
